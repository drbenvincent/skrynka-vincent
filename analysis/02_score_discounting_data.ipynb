{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score discounting data\n",
    "\n",
    "The goal of this notebook is to score the raw delay discounting data. \n",
    "\n",
    "Our primary analysis is intended to focus upon the $\\log(k)$ parameter of the Hyperbolic discount function (Mazur, 1987). This was chosen because it only has one parameter, but is also well known to provide good empirical fits to discounting data.\n",
    "\n",
    "\n",
    "\n",
    "For the sake of completeness and to test the robustness of the findings, we also conduct analyses based upon fits of multiple discount functions:\n",
    "- Exponential (Samuelson, 1937)\n",
    "- Hyperbolic (Mazur, 1987)\n",
    "- Modified Rachlin (Vincent & Stewart, 2019)\n",
    "- Hyperboloid (Myerson & Green, 1995).\n",
    "\n",
    "Because these discount functions have different numbers of parameters, we must a) compare them based upon a single metric, and b) use analysis methods which control for model complexity (e.g. number of parameters). We achieve the former by using the Area Under Curve (AUC) metric (Myerson, Green, & Warusawitharana, 2001), and the latter by using AIC and BIC information criterion metrics.\n",
    "\n",
    "The adaptive delay discounting procedure used was an early version of that developed by Vincent & Rainforth (pre-print). It's goal is to adaptively choose discount functions so as to reduce our uncertainty about the $\\log(k)$ discount rate parameter of the hyperbolic function as efficientl as possible. So while the obtained behavioural data _can_ be used to estimate the parameters of other discount functions, we must bear in mind that distinguishing between different discount functions was not the aim of our adaptive procedure.\n",
    "\n",
    "We output a long-format .csv file for each of the discount functions. We conduct Bayesian parameter estimation using mulitple discount functions (Exponential, Hyperbolic, ...) and then calculate the Area Under Curve (AUC). This is _non_ hierarchical, in that we do parameter estimation for each raw delay discounting experiment (ie participant/condition/commodity combination) separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "np.random.seed(123)  # Initialize random number generator\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyMC3 & my models\n",
    "import pymc3 as pm\n",
    "import models\n",
    "from plot import plot_data\n",
    "\n",
    "# Autoreload imported modules. Convenient while I'm developing the code.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of filenames of the raw delay discounting data files that we want to iterate over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_folder = f'../data/discounting/'\n",
    "search_string = root_data_folder + '*.txt'\n",
    "files = glob(search_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our set of models which we will use to conduct parameter estimation with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = {'Exponential': models.Exponential, \n",
    "              'Hyperbolic': models.Hyperbolic,\n",
    "              'ModifiedRachlin': models.ModifiedRachlin, \n",
    "              'Hyperboloid': models.Hyperboloid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_options = {'tune': 1_000, 'draws': 2_000,\n",
    "                  'chains': 4, 'cores': 4,\n",
    "                  'nuts_kwargs': {'target_accept': 0.95}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up our output location for saved files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'fits/'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over all the raw files, fitting multiple models. As we go, we build up a list of dataframes which (at the end) is concatenated into a single dataframe. This will be in long format, so each row corresponds to a single file (ie participant, commodity, condition combination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(fname):\n",
    "    path, file = os.path.split(fname)\n",
    "    initials, commodity, condition, date, time = file.split('-')\n",
    "    return (initials, commodity, condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over files and discount functions\n",
    "ðŸ”¥ This will take a few hours to compute ðŸ”¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists for our discount functions\n",
    "fit_data = [[], [], [], []]\n",
    "\n",
    "for i, fname in enumerate(files):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    initials, commodity, condition = parse_filename(fname)\n",
    "    data = pd.read_csv(fname, sep='\\t')\n",
    "    \n",
    "    plot_data(data, ax[0])\n",
    "    \n",
    "    # build a list of each model\n",
    "    models = [None, None, None, None]\n",
    "    \n",
    "    for m, (model_name, model_build_func) in enumerate(model_list.items()):\n",
    "        models[m] = model_build_func(data)\n",
    "        models[m].fit(sample_options)\n",
    "        models[m].plot(ax[0], label=model_name)\n",
    "\n",
    "        # save info to appropriate element in fit_data list\n",
    "        info = {'id': [initials], \n",
    "                'commodity': [commodity],\n",
    "                'condition': [condition],\n",
    "                'model': [model_name], \n",
    "                'log_loss': [np.median(models[m].metrics['log_loss'])],\n",
    "                'AUC': [models[m].metrics['AUC']],\n",
    "                'WAIC': [models[m].metrics['WAIC']], \n",
    "                'roc_auc': [models[m].metrics['roc_auc']]}\n",
    "        # add params\n",
    "        params = models[m].mean_parameters()\n",
    "        # merge params into info\n",
    "        info.update(params)\n",
    "        row = pd.DataFrame.from_dict(info)\n",
    "        display(row)\n",
    "        fit_data[m].append(row)\n",
    "\n",
    "    # update and export data + model fit figure\n",
    "    ax[0].legend()\n",
    "    ax[0].set_title(f'{initials}, {commodity}, {condition}')\n",
    "    ax[0].set_xlabel('delay [days]')\n",
    "    ax[0].set_ylabel('discount fraction')\n",
    "\n",
    "    # now we've got all models for this file, we can do model comparison\n",
    "    df_comp_WAIC = pm.compare({models[0].model: models[0].trace,\n",
    "                               models[1].model: models[1].trace, \n",
    "                               models[2].model: models[2].trace,\n",
    "                               models[3].model: models[3].trace})\n",
    "    \n",
    "    display(df_comp_WAIC)\n",
    "\n",
    "    # save WAIC model comparison plot\n",
    "    waic_plot = pm.compareplot(df_comp_WAIC, ax=ax[1])\n",
    "\n",
    "    savename = f'fits/{initials}_{commodity}_{condition}.pdf'\n",
    "    plt.savefig(savename, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    \n",
    "# Now concatenate and export the .csv files for each model\n",
    "for m, (model_name, _) in enumerate(model_list.items()):\n",
    "    fit_data[m] = pd.concat(fit_data[m], ignore_index=True)\n",
    "    fit_data[m].to_csv(f'parameter_estimation_{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Mazur, J. E. (1987). An adjusting procedure for studying delayed reinforcement. In M. L. Commons, J. A. Nevin, & H. Rachlin (Eds.), Quantitative analyses of behavior (pp. 55â€“73). Hillsdale, NJ: Erlbaum.\n",
    "\n",
    "Myerson, J., & Green, L. (1995). Discounting of delayed rewards: Models of individual choice. Journal of the Experimental Analysis of Behavior, 64 (3), 263â€“276\n",
    "\n",
    "Myerson, J., Green, L., & Warusawitharana, M. (2001). Area under the curve as a measure of discounting. Journal of the Experimental Analysis of Behavior, 76(2), 235â€“243. http://doi.org/10.1901/jeab.2001.76-235\n",
    "\n",
    "Samuelson, P. A. (1937). A note on measurement of utility. The Review of Economic Studies, 4(2), 155.\n",
    "\n",
    "Vincent & Rainforth (pre-print) \n",
    "\n",
    "Vincent, B. T., & Stewart, N. (2019). The case of muddled units in temporal discounting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
